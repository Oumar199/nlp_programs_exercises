{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Récupération et exploration du corpus d'un texte\n",
    "--------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est important de bien identifier 4 termes incontournables : \n",
    "\n",
    "- le corpus : un ensemble de documents (des textes dans notre cas), regroupés dans une optique ou dans une thématique précise. \n",
    "\n",
    "- un document : la notion de document fait référence à un texte appartenant au corpus, mais indépendant des autres textes. Il peut être constitué d'une ou plusieurs phrases, un ou plusieurs paragraphes.\n",
    "\n",
    "- un token : le terme token désigne généralement un mot et/ou un élément de ponctuation. La phrase \"Hello World!\" comprend donc 3 tokens. \n",
    "\n",
    "- le vocabulaire : il s'agit de l'ensemble des tokens distincts présents dans l'ensemble du corpus."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les documents .txt, .doc, .csv, .xls, cela semble facile, mais comment faire pour les documents .pdf ?  \n",
    "\n",
    "Transformer les documents .pdf en .txt n'est pas une chose facile. Ce traitement spécifique à un nom, cela s'appelle OCR :  Optical Character Recognition. \n",
    "\n",
    "Heureusement, il existe de nombreux packages disponibles, comme par exemple Tesseract."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons aborder trois pré-traitements à effectuer:\n",
    "- Récupération et traitement du corpus\n",
    "- La tokénisation du corpus\n",
    "- La normalisation et la construction du dictionnaire"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le cycle de traitement d'un corpus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pipeline](pipeline1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupération du corpus de texte"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention au format d'encodage de vos fichiers texte qui peuvent mener à des erreurs faciles à éviter. On favorisera l'UTF-8 (encodage universel) très utilisé et qui permet l'usage des accents."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![corpus_structure](text-corpus-structure.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prenons science_fiction et humor\n",
    "words = nltk.corpus.brown.words(categories=['humor', 'science_fiction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36165"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping des descriptions des artistes francais disponible sur wikipedia https://fr.wikipedia.org/wiki/Liste_d%27artistes_fran%C3%A7ais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as rq\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 299/399 [10:29<03:34,  2.14s/it]"
     ]
    }
   ],
   "source": [
    "url = r\"https://fr.wikipedia.org/wiki/Liste_d%27artistes_fran%C3%A7ais\"\n",
    "\n",
    "with rq.get(url) as response:\n",
    "    if response.ok:\n",
    "        text = response.text\n",
    "        \n",
    "        artistes = {}\n",
    "        \n",
    "        try:\n",
    "            dom = BeautifulSoup(text, \"lxml\")\n",
    "            artistes_li = dom.select('div[id = \"mw-content-text\"] li a[href *= \"/wiki\"]')\n",
    "            \n",
    "            # après récupération des listes nous pouvons passer à celles des noms et des descriptions\n",
    "            \n",
    "            for artiste in tqdm(artistes_li):\n",
    "                # recuperation du nom\n",
    "                name = artiste.text\n",
    "                \n",
    "                # recuperation de la description a partir du lien wikipedia\n",
    "                link = artiste.get('href')\n",
    "                \n",
    "                full_link = \"\".join([\"https://fr.wikipedia.org/\", link])\n",
    "                \n",
    "                descriptions = []\n",
    "                \n",
    "                with rq.get(full_link) as response2:\n",
    "                    \n",
    "                    if response2.ok:\n",
    "                        \n",
    "                        text2 = response2.text\n",
    "                    \n",
    "                        try:\n",
    "                            dom2 = BeautifulSoup(text2, \"lxml\")\n",
    "\n",
    "                            paragraphs = dom.select('p')\n",
    "                        \n",
    "                            for paragraph in paragraphs:\n",
    "                                descriptions.append(paragraph.text)\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"Error part 2 {e}\")\n",
    "                \n",
    "                if len(descriptions) != 0:\n",
    "                     artistes[name] = \"\\n\".join(descriptions)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artistes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration du text: tokenisation et analyse des fréquences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bonjour',\n",
       " ',',\n",
       " 'je',\n",
       " 'suis',\n",
       " 'un',\n",
       " 'texte',\n",
       " \"d'exemple\",\n",
       " 'pour',\n",
       " 'le',\n",
       " 'cours',\n",
       " \"d'Openclassrooms\",\n",
       " '.',\n",
       " 'Soyez',\n",
       " 'attentifs',\n",
       " 'à',\n",
       " 'ce',\n",
       " 'cours',\n",
       " '!']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Bonjour, je suis un texte d'exemple pour le cours d'Openclassrooms. Soyez attentifs à ce cours !\"\n",
    "\n",
    "nltk.word_tokenize(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les accents sont affichés encodés mais ce n'est pas très grave, on peut revenir à un affichage normal si on le souhaite.\n",
    "\n",
    "Le fait d'essayer d'harmoniser les tokens est un processus nommé « normalisation ». \n",
    "\n",
    "https://www.debuggex.com/cheatsheet/regex/python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m tokenizer \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mRegexpTokenizer(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m tokenizer\u001b[39m.\u001b[39mtokenize(text\u001b[39m.\u001b[39;49mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mlower())\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "tokenizer.tokenize(text.decode('utf-8').lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grammarrules-R9LSO1qf-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "350e372a59c6a2174c4996d01ba34ddc044542d9850a5b2a01946a38bca0736a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
