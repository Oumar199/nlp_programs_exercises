{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-grams \n",
    "----------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## French Part"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les n grams sont utilisés dans le but de connaître le contexte d'un mot ou d'un groupe de mots dans un texte. Autrement dit, considérons une intelligence artificielle (ab. IA) qui souhaite connaître le sens de chaque mot dans un texte dans le but de prédire le sentiment que l'auteur a voulu exprimer. l'IA ne sera pas capable d'effectuer cette tâche sans une certaine connaîssance de la grammaire formelle. Cela est impossible à cause du nombre important de règles à adopter. C'est là que les n-grams vont entrer en jeu. Aulieu d'apprendre les règles de la grammaire francaise, l'IA va devoir essayer de reconnaitre des parties du texte qu'on lui fourni et non la globalité de ce dernier."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Définition du n-gram d'un texte**: Le n-gram est un ensemble quelconque de n éléments contigues d'un texte. Cet élément peut-être :\n",
    "- Un mot : On divise le texte en un ensemble de parties constituées de n mots par pas de 1 mot.\n",
    "- Un caractère: On divise le texte en un ensemble de parties constituées de n caractères par pas de 1 caractère. \n",
    "\n",
    "Les éléments récupérés peuvent contenir ou pas de ponctuation(s) suivant la tâche que l'on souhaite accomplir."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Définition de la tokénisation**: La tokénisation est la séparation d'un texte en un ensemble d'éléments unitaires. Il peut s'agîr :\n",
    "- De la séparation du texte en mots,\n",
    "- Ou en phrases séparées par des points. On peut effectuer la séparation par phrase pour ensuite effectuer la séparation par mot pour éviter les ponctuations ou pour analyser chaque phrase séparément."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les éléments récupérés après tokénisation d'un texte (*tokens*) peuvent servir pour la constitution des n-grams."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Les étiquettes**: Les étiquettes, en anglais pos-tag, représentent la grammaire hors contexte d'un texte à partir de ces tokens. Elle nous permettront notamment de choisir les n-grams ne contenant pas de ponctuation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exemple simple**: Effectuons un exemple simple avec `nltk`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importons la librairie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialisons un texte constituées de deux phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "texte = \"Les papiers sont stériles; L'audace et la romance semblent être passées à jamais dans le monde criminel. Pouvez-vous alors me demander si je suis prêt à examiner un problème nouveau, aussi insignifiant soit-il?\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passons à la tokénisation du texte (en phrases puis en mots) sans considérer la ponctuation\n",
    "\n",
    "Vu que nous utilisons un texte en francais, nous allons donc ajouter l'argument `language = \"french\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phrases\n",
    "phrases = nltk.sent_tokenize(texte, language=\"french\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mots\n",
    "mots = []\n",
    "ponctuations = \".,?!;\"\n",
    "\n",
    "# utilisation de tokéniseur utilisant des expressions régulières\n",
    "tokenizer = nltk.RegexpTokenizer(r\"[\\w']+\") # récupère uniquement les mots\n",
    "\n",
    "for i, phrase in enumerate(phrases):\n",
    "    mots.append(tokenizer.tokenize(phrase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Les',\n",
       "  'papiers',\n",
       "  'sont',\n",
       "  'stériles',\n",
       "  \"L'audace\",\n",
       "  'et',\n",
       "  'la',\n",
       "  'romance',\n",
       "  'semblent',\n",
       "  'être',\n",
       "  'passées',\n",
       "  'à',\n",
       "  'jamais',\n",
       "  'dans',\n",
       "  'le',\n",
       "  'monde',\n",
       "  'criminel'],\n",
       " ['Pouvez',\n",
       "  'vous',\n",
       "  'alors',\n",
       "  'me',\n",
       "  'demander',\n",
       "  'si',\n",
       "  'je',\n",
       "  'suis',\n",
       "  'prêt',\n",
       "  'à',\n",
       "  'examiner',\n",
       "  'un',\n",
       "  'problème',\n",
       "  'nouveau',\n",
       "  'aussi',\n",
       "  'insignifiant',\n",
       "  'soit',\n",
       "  'il']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous obtenons ci-dessus une liste de liste de mots pour une phrase du texte donné."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici une fonction qui récupère les n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_grams(tokens: list, n: int = 2):\n",
    "    n_grams = []\n",
    "    for i in range(len(tokens) - n):\n",
    "        n_grams.append(\" \".join(tokens[i: i+n])) \n",
    "    return n_grams       "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons pouvoir passer à l'affichage des n-grams:\n",
    "\n",
    "- D'abord pour la première phrase par pas de 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Les papiers',\n",
       " 'papiers sont',\n",
       " 'sont stériles',\n",
       " \"stériles L'audace\",\n",
       " \"L'audace et\",\n",
       " 'et la',\n",
       " 'la romance',\n",
       " 'romance semblent',\n",
       " 'semblent être',\n",
       " 'être passées',\n",
       " 'passées à',\n",
       " 'à jamais',\n",
       " 'jamais dans',\n",
       " 'dans le',\n",
       " 'le monde']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_n_grams(mots[0], 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ensuite pour la deuxième phrase par pas de 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pouvez vous alors',\n",
       " 'vous alors me',\n",
       " 'alors me demander',\n",
       " 'me demander si',\n",
       " 'demander si je',\n",
       " 'si je suis',\n",
       " 'je suis prêt',\n",
       " 'suis prêt à',\n",
       " 'prêt à examiner',\n",
       " 'à examiner un',\n",
       " 'examiner un problème',\n",
       " 'un problème nouveau',\n",
       " 'problème nouveau aussi',\n",
       " 'nouveau aussi insignifiant',\n",
       " 'aussi insignifiant soit']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_n_grams(mots[1], 3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grammarrules-R9LSO1qf-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "350e372a59c6a2174c4996d01ba34ddc044542d9850a5b2a01946a38bca0736a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
