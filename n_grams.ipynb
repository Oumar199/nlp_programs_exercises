{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-grams \n",
    "----------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## French Part"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les n grams sont utilisés dans le but de connaître le contexte d'un mot ou d'un groupe de mots dans un texte. Autrement dit, considérons une intelligence artificielle (ab. IA) qui souhaite connaître le sens de chaque mot dans un texte dans le but de prédire le sentiment que l'auteur a voulu exprimer. l'IA ne sera pas capable d'effectuer cette tâche sans une certaine connaîssance de la grammaire formelle. Cela est impossible à cause du nombre important de règles à adopter. C'est là que les n-grams vont entrer en jeu. Aulieu d'apprendre les règles de la grammaire francaise, l'IA va devoir essayer de reconnaitre des parties du texte qu'on lui fourni et non la globalité de ce dernier."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Définition du n-gram d'un texte**: Le n-gram est un ensemble quelconque de n éléments contigues d'un texte. Cet élément peut-être :\n",
    "- Un mot : On divise le texte en un ensemble de parties constituées de n mots par pas de 1 mot.\n",
    "- Un caractère: On divise le texte en un ensemble de parties constituées de n caractères par pas de 1 caractère. \n",
    "\n",
    "Les éléments récupérés peuvent contenir ou pas de ponctuation(s) suivant la tâche que l'on souhaite accomplir."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Définition de la tokénisation**: La tokénisation est la séparation d'un texte en un ensemble d'éléments unitaires. Il peut s'agîr :\n",
    "- De la séparation du texte en mots,\n",
    "- Ou en phrases séparées par des points. On peut effectuer la séparation par phrase pour ensuite effectuer la séparation par mot pour éviter les ponctuations ou pour analyser chaque phrase séparément."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les éléments récupérés après tokénisation d'un texte (*tokens*) peuvent servir pour la constitution des n-grams."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Les étiquettes**: Les étiquettes, en anglais pos-tag, représentent la grammaire hors contexte d'un texte à partir de ces tokens. Elle nous permettront notamment de choisir les n-grams ne contenant pas de ponctuation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exemple simple**: Effectuons un exemple simple avec `nltk`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importons la librairie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialisons un texte constituées de deux phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "texte = \"Les papiers sont stériles; L'audace et la romance semblent être passées à jamais dans le monde criminel. Pouvez-vous alors me demander si je suis prêt à examiner un problème nouveau, aussi insignifiant soit-il?\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passons à la tokénisation du texte (en phrases puis en mots) sans considérer la ponctuation\n",
    "\n",
    "Vu que nous utilisons un texte en francais, nous allons donc ajouter l'argument `language = \"french\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phrases\n",
    "phrases = nltk.sent_tokenize(texte, language=\"french\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mots\n",
    "mots = []\n",
    "ponctuations = \".,?!;\"\n",
    "\n",
    "# utilisation de tokéniseur utilisant des expressions régulières\n",
    "tokenizer = nltk.RegexpTokenizer(r\"[\\w']+\") # récupère uniquement les mots\n",
    "\n",
    "for i, phrase in enumerate(phrases):\n",
    "    mots.append(tokenizer.tokenize(phrase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Les',\n",
       "  'papiers',\n",
       "  'sont',\n",
       "  'stériles',\n",
       "  \"L'audace\",\n",
       "  'et',\n",
       "  'la',\n",
       "  'romance',\n",
       "  'semblent',\n",
       "  'être',\n",
       "  'passées',\n",
       "  'à',\n",
       "  'jamais',\n",
       "  'dans',\n",
       "  'le',\n",
       "  'monde',\n",
       "  'criminel'],\n",
       " ['Pouvez',\n",
       "  'vous',\n",
       "  'alors',\n",
       "  'me',\n",
       "  'demander',\n",
       "  'si',\n",
       "  'je',\n",
       "  'suis',\n",
       "  'prêt',\n",
       "  'à',\n",
       "  'examiner',\n",
       "  'un',\n",
       "  'problème',\n",
       "  'nouveau',\n",
       "  'aussi',\n",
       "  'insignifiant',\n",
       "  'soit',\n",
       "  'il']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous obtenons ci-dessus une liste de liste de mots pour une phrase du texte donné."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici une fonction qui récupère les n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_grams(tokens: list, n: int = 2):\n",
    "    n_grams = []\n",
    "    for i in range(len(tokens) - n):\n",
    "        n_grams.append(\" \".join(tokens[i: i+n])) \n",
    "    return n_grams       "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons pouvoir passer à l'affichage des n-grams:\n",
    "\n",
    "- D'abord pour la première phrase par pas de 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Les papiers',\n",
       " 'papiers sont',\n",
       " 'sont stériles',\n",
       " \"stériles L'audace\",\n",
       " \"L'audace et\",\n",
       " 'et la',\n",
       " 'la romance',\n",
       " 'romance semblent',\n",
       " 'semblent être',\n",
       " 'être passées',\n",
       " 'passées à',\n",
       " 'à jamais',\n",
       " 'jamais dans',\n",
       " 'dans le',\n",
       " 'le monde']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_n_grams(mots[0], 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ensuite pour la deuxième phrase par pas de 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pouvez vous alors',\n",
       " 'vous alors me',\n",
       " 'alors me demander',\n",
       " 'me demander si',\n",
       " 'demander si je',\n",
       " 'si je suis',\n",
       " 'je suis prêt',\n",
       " 'suis prêt à',\n",
       " 'prêt à examiner',\n",
       " 'à examiner un',\n",
       " 'examiner un problème',\n",
       " 'un problème nouveau',\n",
       " 'problème nouveau aussi',\n",
       " 'nouveau aussi insignifiant',\n",
       " 'aussi insignifiant soit']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_n_grams(mots[1], 3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Markov assumption (hypothèse de Markov)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après l'obtention des n-grams il nous est possible de prédire le prochain mot dans une séquence à partir des précédents mots la composant. Pour cela on doit calculer, pour chaque possible prochain mot, la probabilité que celui occure sachant que les mots précédents font déja partis de la séquence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple :\n",
    "\n",
    "Pour calculer la probabilité que le prochain mot de la sous séquence \"L'audace et la romance semblent être passées à jamais dans le monde\" soit le mot \"criminel\" nous aurons:\n",
    "\n",
    "$$\n",
    "P(prochain\\_mot = \\text{\"criminel\"} / \\text{sous-séquence}) =\n",
    "$$\n",
    "$$\n",
    "\\frac{\\text{nombre d'occurences \"L'audace et la romance semblent être passées à jamais dans le monde criminel\"}}{\\text{nombre d'occurences \"L'audace et la romance semblent être passées à jamais dans le monde\"}}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette méthode est peut recommendable car elle peut biaiser notre résultat car dans la plupart des cas le nombre d'occurences d'une sous-séquence aussi grande est très faible (surtout dans notre cas où les phrases ont une composition très complexe)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour pallier à ce problème nous allons à la place utiliser les n-grams pour ne prendre comme sous-séquence que les n mots précédents le possible prochain mot. On utilisera, dans cette dernière option, l'hypothèse de Markov selon laquelle la probabilité qu'un événement se produit ne dépend que des $o$ derniers événements et non de tous les événements qui se sont produits depuis le début. $o$ représente l'ordre de Markov qui est le nombre d'événements que l'on considère comme influentes pour produire le prochain événement."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple : Pour l'exemple précédent aulieu de prendre en compte toute la sous-séquence \"L'audace et la romance semblent être passées à jamais dans le monde\", si nous utilisons un bigramme (2-grams), nous ne prendrons que les deux derniers mots de la sous-séquence \"le monde\". Si par contre nous utilisons un trigramme (3-grams), nous aurons comme sous-séquence résultante \"dans le monde\". Donc la probabilité que le prochain soit \"criminel\" peut-être obtenue avec le prochain calcul:\n",
    "\n",
    "$$\n",
    "P(prochain\\_mot = \\text{\"criminel\"} / \\text{bigrammes = \"le monde\"}) = \\frac{\\text{nombre d'occurences \"le monde criminel\"}}{\\text{nombre d'occurences \"le monde\"}}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effectuons un autre exemple avec la librairie `spacy` qui contient toutes les fonctions dont nous aurons utiliser pour calculer les n-grams, tokéniser les documents et calculer les probabilités/distributions en vue de la prédiction du prochain mot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargeons le modèle francais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"fr_core_news_md\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisons une fonction de tokénisation que nous avions créé pour les tweets. Nous allons fournir le texte que nous avions utiliser au début."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_project.processing.utils import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, pos_tags = tokenization(nlp, [texte], rm_stopwords=False, lemmatize=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode retourne une liste de tokens pour chaque document et nous n'avons fourni qu'un seul document donc nous obtenons une seule liste que nous allons récupérer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokens[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Récupérons à présent les bigrammes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = get_n_grams(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['les papiers',\n",
       " 'papiers sont',\n",
       " 'sont audace',\n",
       " 'audace et',\n",
       " 'et la',\n",
       " 'la romance',\n",
       " 'romance semblent',\n",
       " 'semblent jamais',\n",
       " 'jamais dans',\n",
       " 'dans le',\n",
       " 'le monde',\n",
       " 'monde criminel',\n",
       " 'criminel pouvez',\n",
       " 'pouvez alors',\n",
       " 'alors me',\n",
       " 'me demander',\n",
       " 'demander si',\n",
       " 'si je',\n",
       " 'je suis',\n",
       " 'suis examiner',\n",
       " 'examiner un',\n",
       " 'un nouveau',\n",
       " 'nouveau aussi',\n",
       " 'aussi insignifiant']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grammarrules-R9LSO1qf-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "350e372a59c6a2174c4996d01ba34ddc044542d9850a5b2a01946a38bca0736a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
